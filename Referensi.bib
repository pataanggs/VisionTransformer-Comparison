@inproceedings{dosovitskiy2020image,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2010.11929}
}

@inproceedings{liu2021swin,
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {10012--10022},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.14030}
}

@inproceedings{touvron2021training,
  title        = {Training Data-efficient Image Transformers \& Distillation through Attention},
  author       = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle    = {International Conference on Machine Learning (ICML)},
  pages        = {10347--10357},
  year         = {2021},
  organization = {PMLR},
  url          = {https://arxiv.org/abs/2012.12877}
}

@inproceedings{he2022masked,
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {16000--16009},
  year      = {2022},
  url       = {https://arxiv.org/abs/2111.06377}
}

@article{vaswani2017attention,
  title   = {Attention is All you Need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume  = {30},
  year    = {2017},
  url     = {https://arxiv.org/abs/1706.03762}
}

@inproceedings{krizhevsky2012imagenet,
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {25},
  year      = {2012}
}

@inproceedings{he2016deep,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {770--778},
  year      = {2016}
}

@article{khan2022transformers,
  title     = {Transformers in Vision: A Survey},
  author    = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal   = {ACM Computing Surveys},
  volume    = {54},
  number    = {10s},
  pages     = {1--41},
  year      = {2022},
  publisher = {ACM New York, NY},
  url       = {https://arxiv.org/abs/2101.01169}
}

@article{han2022survey,
  title     = {A Survey on Vision Transformer},
  author    = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and others},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {45},
  number    = {1},
  pages     = {87--110},
  year      = {2022},
  publisher = {IEEE},
  url       = {https://arxiv.org/abs/2012.12556}
}

@inproceedings{deng2009imagenet,
  title        = {ImageNet: A Large-scale Hierarchical Image Database},
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {248--255},
  year         = {2009},
  organization = {IEEE}
}

@article{steiner2021train,
  title   = {How to Train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  author  = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal = {arXiv preprint arXiv:2106.10270},
  year    = {2021},
  url     = {https://arxiv.org/abs/2106.10270}
}

@inproceedings{bao2021beit,
  title     = {BEiT: BERT Pre-training of Image Transformers},
  author    = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2106.08254}
}

@article{chu2021twins,
  title   = {Twins: Revisiting the Design of Spatial Attention in Vision Transformers},
  author  = {Chu, Xiangxiang and Tian, Zhi and Wang, Yuqing and Zhang, Bo and Ren, Haibing and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume  = {34},
  pages   = {9355--9366},
  year    = {2021},
  url     = {https://arxiv.org/abs/2104.13840}
}

@inproceedings{yuan2021tokens,
  title     = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},
  author    = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {558--567},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.11986}
}

@article{raghu2021vision,
  title   = {Do Vision Transformers See Like Convolutional Neural Networks?},
  author  = {Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume  = {34},
  pages   = {12116--12128},
  year    = {2021},
  url     = {https://arxiv.org/abs/2108.08810}
}

@inproceedings{wu2021cvt,
  title     = {CvT: Introducing Convolutions to Vision Transformers},
  author    = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {22--31},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.15808}
}

@article{dosovitskiy2021mlp,
  title   = {MLP-Mixer: An All-MLP Architecture for Vision},
  author  = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume  = {34},
  pages   = {24261--24272},
  year    = {2021},
  url     = {https://arxiv.org/abs/2105.01601}
}

@article{liu2022swinv2,
  title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},
  author  = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages   = {12009--12019},
  year    = {2022},
  url     = {https://arxiv.org/abs/2111.09883}
}