% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with deep convolutional neural networks,'' in \emph{Advances in Neural Information Processing Systems (NeurIPS)}, vol.~25, 2012.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016, pp. 770--778.

\bibitem{vaswani2017attention}
\BIBentryALTinterwordspacing
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{Advances in Neural Information Processing Systems (NeurIPS)}, vol.~30, 2017. [Online]. Available: \url{https://arxiv.org/abs/1706.03762}
\BIBentrySTDinterwordspacing

\bibitem{dosovitskiy2020image}
\BIBentryALTinterwordspacing
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.}, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \emph{International Conference on Learning Representations (ICLR)}, 2021. [Online]. Available: \url{https://arxiv.org/abs/2010.11929}
\BIBentrySTDinterwordspacing

\bibitem{liu2021swin}
\BIBentryALTinterwordspacing
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2021, pp. 10\,012--10\,022. [Online]. Available: \url{https://arxiv.org/abs/2103.14030}
\BIBentrySTDinterwordspacing

\bibitem{touvron2021training}
\BIBentryALTinterwordspacing
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou, ``Training data-efficient image transformers \& distillation through attention,'' in \emph{International Conference on Machine Learning (ICML)}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 10\,347--10\,357. [Online]. Available: \url{https://arxiv.org/abs/2012.12877}
\BIBentrySTDinterwordspacing

\bibitem{he2022masked}
\BIBentryALTinterwordspacing
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick, ``Masked autoencoders are scalable vision learners,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 16\,000--16\,009. [Online]. Available: \url{https://arxiv.org/abs/2111.06377}
\BIBentrySTDinterwordspacing

\bibitem{coates2011analysis}
\BIBentryALTinterwordspacing
A.~Coates, A.~Ng, and H.~Lee, ``An analysis of single-layer networks in unsupervised feature learning,'' in \emph{Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS)}.\hskip 1em plus 0.5em minus 0.4em\relax JMLR Workshop and Conference Proceedings, 2011, pp. 215--223, official STL-10 Dataset Source. [Online]. Available: \url{http://cs.stanford.edu/~acoates/stl10/}
\BIBentrySTDinterwordspacing

\bibitem{lee2021vision}
\BIBentryALTinterwordspacing
S.~H. Lee, S.~Lee, and B.~C. Song, ``Vision transformers for small-size datasets,'' in \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 2022, pp. 1992--2001. [Online]. Available: \url{https://arxiv.org/abs/2112.13492}
\BIBentrySTDinterwordspacing

\bibitem{loshchilov2017decoupled}
\BIBentryALTinterwordspacing
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in \emph{International Conference on Learning Representations (ICLR)}, 2019, primary reference for AdamW optimizer used in ViT/Swin/DeiT. [Online]. Available: \url{https://arxiv.org/abs/1711.05101}
\BIBentrySTDinterwordspacing

\bibitem{caron2021emerging}
\BIBentryALTinterwordspacing
M.~Caron, H.~Touvron, I.~Misra, H.~J{\'e}gou, J.~Mairal, P.~Bojanowski, and A.~Joulin, ``Emerging properties in self-supervised vision transformers,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2021, pp. 9650--9660. [Online]. Available: \url{https://arxiv.org/abs/2104.14294}
\BIBentrySTDinterwordspacing

\bibitem{xiao2021early}
\BIBentryALTinterwordspacing
T.~Xiao, M.~Singh, E.~Mintun, T.~Darrell, P.~Doll{\'a}r, and R.~Girshick, ``Early convolutions help transformers see better,'' \emph{Advances in Neural Information Processing Systems (NeurIPS)}, vol.~34, pp. 30\,392--30\,400, 2021. [Online]. Available: \url{https://arxiv.org/abs/2106.14881}
\BIBentrySTDinterwordspacing

\end{thebibliography}
